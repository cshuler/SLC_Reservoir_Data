{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 85%; }\n",
       "    div#maintoolbar-container { width: 99%; } </style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "pd.options.mode.chained_assignment = None  # default='warn' #turn off the SettingWithCopyWarning\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "from matplotlib.dates import YearLocator, DateFormatter\n",
    "%matplotlib notebook\n",
    "\n",
    "import datetime\n",
    "from dateutil import parser, rrule\n",
    "from datetime import datetime, time, date\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(data=\"\"\" <style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 85%; }\n",
    "    div#maintoolbar-container { width: 99%; } </style> \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files in from online source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RES_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RES_ID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#gather UHSLC ID +9\u001b[39;00m\n\u001b[0;32m      2\u001b[0m meta \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreservoir_basin_data080923.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m UHSLC_ID \u001b[38;5;241m=\u001b[39m \u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRES_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist() ; UHSLC_ID \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m UHSLC_ID \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#combine all data in dictionary\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ALL_WL_data \u001b[38;5;241m=\u001b[39m{}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RES_ID'"
     ]
    }
   ],
   "source": [
    "#gather UHSLC ID +9\n",
    "meta = pd.read_csv('reservoir_basin_data080923.csv')\n",
    "UHSLC_ID = meta['RES_ID'].tolist() ; UHSLC_ID = [x for x in UHSLC_ID if str(x) != 'nan'] \n",
    "\n",
    "\n",
    "#combine all data in dictionary\n",
    "ALL_WL_data ={}\n",
    "for ID in UHSLC_ID: \n",
    "    data = pd.read_csv('https://uhslc.soest.hawaii.edu/reservoir/'  + ID  +  '-full.csv')\n",
    "    ALL_WL_data[ID] = data\n",
    "    print(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in files from USGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(os.path.join(\"..\", \"Data/usgs_051724\")):\n",
    "    df= pd.read_csv(os.path.join(\"..\", \"Data/usgs_051724\", file))\n",
    "    df = df.rename(columns={'datetime': 'date', '00065': 'data'})\n",
    "    ID = file.split('.')[0]\n",
    "    ALL_WL_data[ID] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resample to DAILY data, shift time, convert to right units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_dic = {}\n",
    "\n",
    "for ID in ALL_WL_data:\n",
    "    print(ID)\n",
    "    data = ALL_WL_data[ID]\n",
    "    ALL_WL_data[ID]['date'] = pd.to_datetime(ALL_WL_data[ID]['date'])\n",
    "    data = data.replace(-66577, np.nan)\n",
    "    data = data.replace(131071, np.nan)\n",
    "    data = data.replace(-999999.00, np.nan)\n",
    "    \n",
    "    # Data now comes in with a timezone aware index and in GMT, as its easier to work in HST, just remove timezone and convert to HST \n",
    "    data = data.set_index('date')\n",
    "    data = data.tz_localize(None)         # Scrub out the Timezone awareness of the index \n",
    "    data.shift(-10, freq='H')             # Shift the timestamp back 10 hrs \n",
    "    \n",
    "    # Remove outliers based on a certain number of standard deviations from the mean \n",
    "    # Probably could use a more advanced filtering method later if need\n",
    "    STDs = 3\n",
    "    data = data[((data['data'] - data['data'].mean()) / data['data'].std()).abs() < STDs]\n",
    "    \n",
    "    # Convert from decimal feet to ft. \n",
    "    if 'E' in ID:\n",
    "        data['data'] = data['data']/100\n",
    "    \n",
    "    #resample data to daily \n",
    "    data_daily =  data.resample('1D').mean() \n",
    "    \n",
    "    # record daily files in a dictionary of dataframes\n",
    "    daily_dic[ID] = data_daily\n",
    "    \n",
    "daily_dic.keys()\n",
    "\n",
    "\n",
    "## Import metadata\n",
    "Station_Meta = pd.read_csv(\"Gauged_reservoirs_META.csv\", index_col=0)\n",
    "# fix this weird data...\n",
    "#daily_dic['WL_EDD08400.csv']['data'] = daily_dic['WL_EDD08400.csv']['data'] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull together some stats for each reservoir\n",
    "ids = []\n",
    "mean = []\n",
    "minimum = [] \n",
    "maximum = []\n",
    "stdev = []\n",
    "median = []\n",
    "\n",
    "for key in daily_dic:\n",
    "    ids.append(key)\n",
    "    \n",
    "    mean_= daily_dic[key].data.mean(skipna = True)\n",
    "    mean.append(mean_)\n",
    "    \n",
    "    minimum_ = daily_dic[key].data.min()\n",
    "    minimum.append(minimum_)\n",
    "    \n",
    "    maximum_ = daily_dic[key].data.max()\n",
    "    maximum.append(maximum_)\n",
    "    \n",
    "    median_ = daily_dic[key].data.median()\n",
    "    median.append(median_)\n",
    "    \n",
    "    stdev_ = daily_dic[key].data.std()\n",
    "    stdev.append(stdev_)\n",
    "\n",
    "#create dataframe\n",
    "stats_dic = {'file_id': ids, 'mean': mean, 'minimum': minimum, 'maximum' : maximum, 'median': median, 'standard_deviation' : stdev}\n",
    "stats_df = pd.DataFrame.from_dict(stats_dic)\n",
    "\n",
    "#get the dlnrid\n",
    "Station_Meta = Station_Meta.set_index('id')\n",
    "dlnrid = []\n",
    "for i in stats_df.file_id:\n",
    "    dlnrid_ = Station_Meta.dlnrid[i]\n",
    "    dlnrid.append(dlnrid_)\n",
    "\n",
    "#add dlnrid\n",
    "stats_df['dlrnid'] = np.nan\n",
    "stats_df['dlrnid'] = dlnrid\n",
    "\n",
    "#save csv file\n",
    "stats_df.to_csv('reservoir_stats.csv')\n",
    "Station_Meta = Station_Meta.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the start date, end date of data, duration, and completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "def reservoir_WL_metadata(WaterLevel_data, WaterLevel_dates):\n",
    "    WaterLevel_dates = pd.to_datetime(WaterLevel_dates)\n",
    "    \n",
    "    #end and start date\n",
    "    end_date = WaterLevel_dates.max()\n",
    "    start_date = WaterLevel_dates.min()\n",
    "    \n",
    "    #calculate total duration of data\n",
    "    duration = (end_date - start_date)/np.timedelta64(1, 'Y')\n",
    "    \n",
    "    #calculate data frequency\n",
    "    datapoint_frequency = WaterLevel_dates[1] - WaterLevel_dates[0]\n",
    "    \n",
    "    #calculate percent completenes\n",
    "    n_total_datas = len(WaterLevel_data)\n",
    "    total_nan = np.count_nonzero(np.isnan(np.array(WaterLevel_data))) #count number of values in array equal to NaN\n",
    "    completeness = 1 - total_nan / n_total_datas\n",
    "    \n",
    "    df = pd.DataFrame({'end_date': end_date , 'start_date' : start_date, 'duration' : duration, 'completeness' : completeness, 'data_timestep' : datapoint_frequency}, index = [0])\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reservoir_WL_completness = {}\n",
    "for file in daily_dic:\n",
    "    #get a list of the water level data and dates recored for function above\n",
    "    WaterLevel_data = daily_dic[file].data\n",
    "    WaterLevel_dates = daily_dic[file].index\n",
    "    \n",
    "    #use the function above to figure out duration, sstart, end, completness\n",
    "    Reservoir_WL_completness[file] = reservoir_WL_metadata(WaterLevel_data, WaterLevel_dates)\n",
    "    \n",
    "#combine all the reservoir data into one variable\n",
    "result = pd.concat(Reservoir_WL_completness).rename_axis(['Reservoir_station_file', 'whatever']).reset_index().drop('whatever', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge the dlnrid into the station meta\n",
    "result['dlnrid'] = np.nan\n",
    "station_merge_meta = Station_Meta[['dlnrid', 'id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the dlnrid into the station meta\n",
    "result['dlnrid'] = np.nan\n",
    "station_merge_meta = Station_Meta[['dlnrid', 'id']].copy()\n",
    "\n",
    "i = 0\n",
    "for ID in result.Reservoir_station_file:\n",
    "    element = ID\n",
    "    index_location = station_merge_meta.index[station_merge_meta['id'] == element].tolist()\n",
    "    dlnrid = station_merge_meta['dlnrid'][index_location]\n",
    "    result.dlnrid[i] = dlnrid[index_location[0]]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('reservoir_data_completeness_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Data Duration and Completeness Graph by STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_info = result\n",
    "\n",
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "\n",
    "# creating color bar stuff\n",
    "my_cmap = plt.cm.get_cmap('inferno_r')\n",
    "data_color = data_info.completeness\n",
    "normmin=0\n",
    "normmax=1\n",
    "data_color = [(x-normmin) / (normmax-normmin) for x in data_color] #see the difference here\n",
    "colors = my_cmap(data_color)\n",
    "\n",
    "#color bar stuff\n",
    "norm = mpl.colors.Normalize(vmin=0,vmax=1)\n",
    "sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.set_label('% Complete', rotation=270,labelpad=25)\n",
    "\n",
    "#bar graphs\n",
    "rects = ax.bar(data_info.dlnrid, data_info.duration, color=colors, edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.yticks(np.arange(0, 20, 2))\n",
    "\n",
    "\n",
    "\n",
    "#the one year line\n",
    "#a = ax.axhline(y = 1, color = 'r', linestyle = '-') \n",
    "\n",
    "#labels and margins\n",
    "plt.title('Duration and Completness of Reservoir Water Level Data', fontweight  = 'bold')\n",
    "ax.set_xlabel('DLNR Reservoir id')\n",
    "ax.set_ylabel('Duration of Data (yr)')\n",
    "\n",
    "\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', which='major', axis='y', linestyle='dashed', alpha = 0.25)\n",
    "\n",
    "plt.savefig('dur_comp_res_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Hydrologic Response Ratings Based on Alert Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hydrologic_response_rating(WaterLevel_data, start_alert_WL, stop_alert_WL):\n",
    "    \n",
    "    hydrologic_response = 'unknown'\n",
    "\n",
    "    #WL goes above the alert level = HAZARD\n",
    "    for i in WaterLevel_data: \n",
    "        if i > start_alert_WL:\n",
    "            hydrologic_response = 'hazard'\n",
    "\n",
    "    #WL never goes above alert level, but does go above the stop level = PROBLEM       \n",
    "    if hydrologic_response  == 'unknown':\n",
    "        for i in WaterLevel_data: \n",
    "            if i > stop_alert_WL:\n",
    "                hydrologic_response  = 'problem'\n",
    "\n",
    "    #WL never goes above the alert or stop levels = SAFE\n",
    "    if hydrologic_response  == 'unknown':\n",
    "        hydrologic_response = 'safe'\n",
    "\n",
    "    if math.isnan(start_alert_WL) == True: \n",
    "        hydrologic_response = 'unknown'\n",
    "    return hydrologic_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrologic_response= {}\n",
    "for file in daily_dic:\n",
    "    \n",
    "    #get the Water level alert levels from the SLC id in meta data table\n",
    "    station_name = file #.split('.')[0].split('_')[1] #split the WL_..... file so that the SLC id is the only letters (remove WL_ and remove .csv)\n",
    "    station_meta_index = np.where(Station_Meta.id == station_name)[0][0] #find the row where the SLC id is used to determine start and stop levels\n",
    "    start_alert_WL = Station_Meta.level_alert_on[station_meta_index] #start alert from meta data table\n",
    "    stop_alert_WL = Station_Meta.level_alert_off[station_meta_index] #stop alert from meta data table\n",
    "\n",
    "    #create a list of Waterlevel data\n",
    "    WaterLevel_data = daily_dic[file]['data']\n",
    "\n",
    "    #determine the station rating using function above\n",
    "    station_rating = hydrologic_response_rating(WaterLevel_data, start_alert_WL, stop_alert_WL)\n",
    "    \n",
    "    #create a dictionary of hydrologic response ratings\n",
    "    hydrologic_response[file] = station_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for hydrologic response to save as a table\n",
    "data_info = data_info.set_index('Reservoir_station_file')\n",
    "\n",
    "dlnrid = []\n",
    "response_rating = []\n",
    "for key in hydrologic_response:\n",
    "    dlnrid.append(data_info['dlnrid'][key])\n",
    "    response_rating.append(hydrologic_response[key])\n",
    "    \n",
    "hydrologic_response_dlnrid = pd.DataFrame({'dlnrid' : dlnrid, 'response_rating' : response_rating})\n",
    "\n",
    "hydrologic_response_dlnrid.to_csv('alert_level_response_ratings.csv')\n",
    "\n",
    "data_info = data_info.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration and Completeness of data graphs with the hazard ones on the left side (data with the highest duration are USGS data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all of the data together and organize by hydrologic response\n",
    "file_hyd_response = pd.DataFrame.from_dict(hydrologic_response, orient = 'index', columns = [ 'hydrologic_response'] )\n",
    "file_hyd_response['filename'] =file_hyd_response.index\n",
    "file_hyd_response.reset_index(drop = True)\n",
    "data_info = data_info.merge(file_hyd_response, left_on = 'Reservoir_station_file', right_on = 'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_info = data_info.sort_values(by = 'hydrologic_response')\n",
    "# #Create Barplot:\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "\n",
    "\n",
    "# rects = ax.bar(data_info.dlnrid, data_info.duration.dt.days / 365, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# plt.xticks(rotation=90, ha='center')\n",
    "\n",
    "\n",
    "# # creating ScalarMappable color stuff\n",
    "# my_cmap = plt.cm.get_cmap('inferno_r')\n",
    "# normmin=0\n",
    "# normmax=1\n",
    "# data_color = data_info.completeness\n",
    "# data_color = [(x-normmin) / (normmax-normmin) for x in data_color] #see the difference here\n",
    "# colors = my_cmap(data_color)\n",
    "\n",
    "# #color bar stuff\n",
    "# norm = mpl.colors.Normalize(vmin=0,vmax=1)\n",
    "# sm = plt.cm.ScalarMappable(cmap=my_cmap, norm=norm)\n",
    "# cbar = plt.colorbar(sm)\n",
    "# cbar.set_label('% Complete', rotation=270,labelpad=25)\n",
    "\n",
    "# #1 year line\n",
    "# #a = ax.axhline(y = 1, color = 'r', linestyle = '-')\n",
    "\n",
    "# #labels\n",
    "# plt.title('Duration and Completness of Reservoir Water Level Data', fontweight  = 'bold')\n",
    "# ax.set_xlabel('DLNR Reservoir id')\n",
    "# ax.set_ylabel('Duration of Data (yr)')\n",
    "# ax.margins(x=0.008, y=0.05)\n",
    "# plt.gcf().subplots_adjust(bottom=0.2)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Hydrologic Response Ratings in all of the reservoirs (ratings are based on alerts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard = 0\n",
    "problem = 0\n",
    "safe = 0\n",
    "unknown = 0\n",
    "\n",
    "for file in hydrologic_response:\n",
    "    if hydrologic_response[file] == 'hazard':\n",
    "        hazard += 1\n",
    "    elif hydrologic_response[file] == 'problem':\n",
    "        problem += 1\n",
    "    elif hydrologic_response[file] == 'unknown':\n",
    "        unknown += 1\n",
    "    else:\n",
    "        safe += 1\n",
    "        \n",
    "        \n",
    "print('hazard: ' + str(hazard))\n",
    "print('problem: ' + str(problem))\n",
    "print('safe: ' + str(safe))\n",
    "print('unknown: ' + str(unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "#bars\n",
    "rects = ax.bar(['High', 'Medium', 'Low', 'NA (USGS)'], [hazard, problem, safe, unknown], color = ['red', 'orange', 'green', 'gray'], edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.bar\n",
    "\n",
    "#labels\n",
    "plt.title('Hydrologic Response Rating', fontweight  = 'bold')\n",
    "ax.set_xlabel('Response Category')\n",
    "ax.set_ylabel('Total Number of Reservoirs')\n",
    "\n",
    "#margins and stuff\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('Hydrologic_Response_by_category.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the highest water level and time during alert, duration of alert, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WL_alert_datas(WaterLevel_data, WaterLevel_data_dates, start_alert_WL, stop_alert_WL):\n",
    "\n",
    "    event_start = []\n",
    "    event_end = [] \n",
    "    duration = []\n",
    "    high_waterlevel = []\n",
    "    date = 'none'\n",
    "    WaterLevel_data_dates = pd.to_datetime(WaterLevel_data_dates) #conver to datetime \n",
    "    highest_water_level_date = []\n",
    "    highest_date = np.nan\n",
    "\n",
    "    i = -1 #start at -1 because loop will add 1 to get the first index of 0\n",
    "    while i != (len(WaterLevel_data) - 1):\n",
    "        i += 1 #add 1 to look at the next/first data point\n",
    "        if WaterLevel_data[i] >= start_alert_WL: #if alert is breached\n",
    "            date = 'start' #starting date of event is found\n",
    "            event_start.append(WaterLevel_data_dates[i]) #starting date of event recorded\n",
    "            highest_water = WaterLevel_data[i] #highest water starts at the time the alert is breached\n",
    "            highest_date= WaterLevel_data_dates[i] #new highest WL date starts at the start of alert\n",
    "        while date == 'start': # while the alert is on\n",
    "            i +=1  #observe the next data point\n",
    "            if i != (len(WaterLevel_data) - 1) and math.isnan(WaterLevel_data[i]) != True: \n",
    "                if WaterLevel_data[i + 1] > highest_water: #if the next data point has a higher WL then it has broken the record for the event\n",
    "                    highest_water = WaterLevel_data[i + 1] #record new high WL if record is broken\n",
    "                    highest_date = WaterLevel_data_dates[i + 1] #record the new highest WL date\n",
    "                if WaterLevel_data[i] <= stop_alert_WL:#if the WL is below the stop alert, then the event is over\n",
    "                    date = 'end' # end the event\n",
    "                    event_end.append(WaterLevel_data_dates[i]) #record the last date in the event\n",
    "                    high_waterlevel.append(highest_water) #record the record breaking high water line\n",
    "                    highest_water_level_date.append(highest_date)\n",
    "            else: \n",
    "                date = 'end' # end the event\n",
    "                event_end.append(WaterLevel_data_dates[i]) #record the last date in the event\n",
    "                high_waterlevel.append(highest_water)\n",
    "                highest_water_level_date.append(highest_date)#record the record breaking high water line\n",
    "\n",
    "    #calculate the duration of each event\n",
    "    for i in range(len(event_end)):\n",
    "        event_duration = event_end[i] - event_start[i]\n",
    "        duration.append(event_duration) \n",
    "    \n",
    "    #create a dataframe to merge all of the data into one\n",
    "    df = pd.DataFrame.from_dict({'event_start': event_start , 'event_end': event_end, 'duration': duration, 'high_waterlevel': high_waterlevel, 'highest_date' : highest_water_level_date})\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WL_alerts= {}\n",
    "for file in daily_dic:\n",
    "    \n",
    "    #get a list of the water level data and dates recored for function above\n",
    "    WaterLevel_data = daily_dic[file].data.tolist()\n",
    "    WaterLevel_data_dates = daily_dic[file].index.tolist()\n",
    "    \n",
    "    #get the Water level alert levels from the SLC id in meta data table\n",
    "    station_name = file#.split('.')[0].split('_')[1] #split the WL_..... file so that the SLC id is the only letters (remove WL_ and remove .csv)\n",
    "    station_meta_index = np.where(Station_Meta.id == station_name)[0][0] #find the row where the SLC id is used to determine start and stop levels\n",
    "    start_alert_WL = Station_Meta.level_alert_on[station_meta_index] #start alert from meta data table\n",
    "    stop_alert_WL = Station_Meta.level_alert_off[station_meta_index] #stop alert from meta data table\n",
    "    \n",
    "    #use the function from above to find alert times and highest water level recorded, and duration of alert\n",
    "    WL_alerts[file] = WL_alert_datas(WaterLevel_data, WaterLevel_data_dates, start_alert_WL, stop_alert_WL)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar graph of the total alerts occurring in each reervoir (UH SLC ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty total alertscolumns\n",
    "data_info['total_alerts'] = np.nan\n",
    "\n",
    "#calculate total alerts in each reservoir\n",
    "for i in range(0, len(data_info)):\n",
    "    filename = data_info['Reservoir_station_file'][i]\n",
    "    total_alerts = len(WL_alerts[filename])\n",
    "    data_info['total_alerts'][i] = total_alerts\n",
    "\n",
    "\n",
    "#sort the reservoirs by highest alert and then pull only the ones that have an alert\n",
    "data_info = data_info.sort_values(by = 'total_alerts')\n",
    "hazard_only = data_info[data_info['total_alerts'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "#bars\n",
    "rects = ax.bar(hazard_only.dlnrid, hazard_only['total_alerts'], edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.bar\n",
    "\n",
    "#labels\n",
    "plt.title('Number of Alerts', fontweight  = 'bold')\n",
    "ax.set_xlabel('DLNR Reservoir id')\n",
    "ax.set_ylabel('Total Alerts')\n",
    "\n",
    "#margins and stuff\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', which='major', axis='y', linestyle='dashed', alpha = 0.25)\n",
    "plt.savefig('number_alerts.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dry seaon vs wet season total alerts (UH SLC ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry = 0 #dry season alerts\n",
    "wet = 0 #wet season alerts\n",
    "\n",
    "alert_start_tally = [] # alert starting dates\n",
    "alert_end_tally = []\n",
    "duration_tally = [] # alert durations\n",
    "reservoir = []\n",
    "for file in WL_alerts:\n",
    "    WL_alerts[file]['event_start'] = pd.to_datetime(WL_alerts[file].event_start)\n",
    "    WL_alerts[file]['event_end'] = pd.to_datetime(WL_alerts[file].event_end)\n",
    "    alert_months = WL_alerts[file].event_start.dt.month\n",
    "\n",
    "    for month in range(len(alert_months)):\n",
    "        alert_start_tally.append(WL_alerts[file]['event_start'][month])\n",
    "        alert_end_tally.append(WL_alerts[file]['event_end'][month])\n",
    "        duration_tally.append(WL_alerts[file]['duration'][month])\n",
    "        reservoir.append(file)\n",
    "        if month > 3 and month < 11:\n",
    "            dry += 1\n",
    "        else:\n",
    "            wet += 1\n",
    "            \n",
    "            \n",
    "print('total DRY season alerts: ' + str(dry))\n",
    "print('total WET season alerts: ' + str(wet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "#bars\n",
    "rects = ax.bar(['Wet', 'Dry'], [wet,dry], color = ['teal', 'tan'], edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.bar\n",
    "\n",
    "#labels\n",
    "plt.title('Seasonal Alerts', fontweight  = 'bold')\n",
    "ax.set_xlabel('Season')\n",
    "ax.set_ylabel('Total Number of Alerts')\n",
    "\n",
    "#margins and stuff\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('seasonal_alert_tally.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert Information Graphs: (duration, timing, location) & (duration, location, season) (UH SLC ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all of the necessary data into a dataframe for the next few graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccreate dataframe to hold everything for graphs\n",
    "alert_durations = pd.DataFrame()\n",
    "\n",
    "#must do weird python pandas thingss to get the number of days as an integer?\n",
    "alert_durations['duration'] = duration_tally\n",
    "alert_durations.duration = alert_durations.duration.dt.days.astype('int16')\n",
    "\n",
    "#create a column of alert starting time and ending\n",
    "alert_durations['alert_start'] = alert_start_tally\n",
    "alert_durations['alert_end'] = alert_end_tally\n",
    "\n",
    "#create a column of reservoir name that the alert occurred in\n",
    "alert_durations['reservoir'] = reservoir\n",
    "\n",
    "#create a column for the state that the alert occurred in\n",
    "alert_durations['dlnrid'] = np.nan\n",
    "alert_durations['state'] = np.nan\n",
    "for i in range(len(alert_durations)):\n",
    "    file = alert_durations.reservoir[i]\n",
    "    SLC_id = file #.split('.')[0].split('_')[1] #split the WL_..... file so that the SLC id is the only letters (remove WL_ and remove .csv)\n",
    "    station_meta_index = np.where(Station_Meta.id == SLC_id)[0][0] #find the row where the SLC id is used to determine start and stop levels\n",
    "    dlnrid = Station_Meta.dlnrid[station_meta_index] \n",
    "    alert_durations['dlnrid'][i] = dlnrid\n",
    "    if alert_durations['dlnrid'][i].split('-')[0] == 'KA':\n",
    "        alert_durations['state'][i] = 1\n",
    "    elif alert_durations['dlnrid'][i].split('-')[0] == 'OA':\n",
    "        alert_durations['state'][i] = 2\n",
    "    elif alert_durations['dlnrid'][i].split('-')[0] == 'MA':\n",
    "        alert_durations['state'][i] = 3\n",
    "        \n",
    "#create a column of month that the alert occurred in using the starting date\n",
    "alert_durations['month']  = alert_durations.alert_start.dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph of Alert Duration, Timing, Location (UH SLC ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot every alert through spatial timing of the alerts, duration, and location\n",
    "KA_alerts = alert_durations.loc[alert_durations['state'] == 1.0] #state \n",
    "OA_alerts = alert_durations.loc[alert_durations['state'] == 2.0]\n",
    "MA_alerts = alert_durations.loc[alert_durations['state'] == 3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot every alert through spatial timing of the alerts, duration, and location\n",
    "\n",
    "#group by state\n",
    "groups = alert_durations.groupby('state')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "#color the dots by state and plot as a scatter\n",
    "for name, group in groups:\n",
    "    ax.scatter(group[\"alert_start\"], group[\"duration\"], marker=\"o\", linestyle=\"\", label=name, alpha = 0.7, s = 60, edgecolor = 'k')\n",
    "\n",
    "#labels\n",
    "ax.set_title('Duration, Date, and Island of Water Level Alerts', fontweight = 'bold')\n",
    "ax.set_ylabel('Duration of Alert (days)')\n",
    "ax.set_xlabel('Start Date of Alert')\n",
    "\n",
    "#x axis lables and what not\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.YearLocator()) #year locator\n",
    "ax.xaxis.set_minor_locator(matplotlib.dates.MonthLocator((4,7,10))) #only months apr, jul, oct\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"\\n%Y\")) \n",
    "ax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\"%b\"))\n",
    "plt.gca().set_xbound(pd.to_datetime('01-01-2018'), pd.to_datetime('07-01-2024')) #x axis limit\n",
    "ax.set_ylim(-1, 65) #y axis limit\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "\n",
    "#Legend\n",
    "ax.legend(['Kauai', 'Oahu', 'Maui'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('duration_date_island_WLAlerts.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Alerts Timeline for each reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "data_info = data_info.set_index('dlnrid')\n",
    "\n",
    "#plot the timelines by state...\n",
    "groups = KA_alerts.groupby(['dlnrid'])\n",
    "for name, group in groups:\n",
    "    ax.hlines(y= group['dlnrid'], xmin = group[\"alert_start\"], xmax = group[\"alert_end\"], colors = 'r', lw = 10)\n",
    "    #plot start and end of data\n",
    "    dlnrid = group['dlnrid'].reset_index(drop = True)[0]\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "groups = OA_alerts.groupby(['dlnrid'])\n",
    "for name, group in groups:\n",
    "    ax.hlines(y= group['dlnrid'], xmin = group[\"alert_start\"], xmax = group[\"alert_end\"], colors = 'r', lw = 10)\n",
    "        #plot start and end of data\n",
    "    dlnrid = group['dlnrid'].reset_index(drop = True)[0]\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "groups = MA_alerts.groupby(['dlnrid'])\n",
    "for name, group in groups:\n",
    "    ax.hlines(y= group['dlnrid'], xmin = group[\"alert_start\"], xmax = group[\"alert_end\"], colors = 'r', lw = 10)\n",
    "    #plot start and end of data\n",
    "    dlnrid = group['dlnrid'].reset_index(drop = True)[0]\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "\n",
    "\n",
    "reservoirs_w_alerts = []    \n",
    "#get a list of all reservoirs with alerts\n",
    "reservoirs_w_alerts.append(KA_alerts.dlnrid.unique().tolist())\n",
    "reservoirs_w_alerts.append(OA_alerts.dlnrid.unique().tolist())\n",
    "reservoirs_w_alerts.append(MA_alerts.dlnrid.unique().tolist())\n",
    "reservoirs_w_alerts = list((itertools.chain.from_iterable(reservoirs_w_alerts)))\n",
    "\n",
    "\n",
    "\n",
    "#plot timeseries of non alerts\n",
    "result = result.set_index('dlnrid')\n",
    "for i in result.index:\n",
    "    if i in reservoirs_w_alerts:\n",
    "        continue\n",
    "    else: \n",
    "        ax.hlines(y= i, xmin = result.start_date[i], xmax = result.end_date[i], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "result = result.reset_index() \n",
    "\n",
    "\n",
    "#### plot  the x axis date format stuff\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.YearLocator())\n",
    "ax.xaxis.set_minor_locator(matplotlib.dates.MonthLocator((4,7,10)))\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"\\n%Y\"))\n",
    "ax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\"%b\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", fontsize = 10)\n",
    "plt.gca().set_xbound(pd.to_datetime('01-01-2018'), pd.to_datetime('07-01-2024')) # Changes x-axis range\n",
    "\n",
    "\n",
    "#title and axis labels\n",
    "ax.set_title('Reservoir Data and Alerts Timeline', fontweight = 'bold', fontsize = 19)\n",
    "ax.set_ylabel('DLNR Reservoir ID', weight = 'bold', fontsize = 14)\n",
    "ax.set_xlabel('Date', weight = 'bold', fontsize = 14)\n",
    "\n",
    "\n",
    "\n",
    "#plot the dash lines \n",
    "a = plt.grid(alpha = 0.45, linestyle = '--', which =  'both', axis = 'x')\n",
    "b = plt.grid(alpha = 0.2, linestyle = '--', which =  'both', axis = 'y')\n",
    "#b = plt.grid(alpha = 0.14, linestyle = '--', color = 'r', axis = 'y', lw = '2')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('WL_alert_timeline.png')\n",
    "\n",
    "data_info = data_info.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reservoirs_w_alerts = list((itertools.chain.from_iterable(reservoirs_w_alerts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph showing Alert Duration, Location, Season (UH SLC ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot every alert through spatial timing of the alerts, duration, and location\n",
    "KA_alerts = alert_durations.loc[alert_durations['state'] == 1.0]\n",
    "OA_alerts = alert_durations.loc[alert_durations['state'] == 2.0]\n",
    "MA_alerts = alert_durations.loc[alert_durations['state'] == 3.0]\n",
    "\n",
    "#plot the points of alerts\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "a = ax.scatter(KA_alerts[\"month\"], KA_alerts[\"duration\"], marker=\"o\", linestyle=\"\", alpha = 0.6, s = 20, edgecolor = 'k')\n",
    "b = ax.scatter(OA_alerts[\"month\"], OA_alerts[\"duration\"], marker=\"o\", linestyle=\"\", alpha = 0.6, s = 20, edgecolor = 'k')\n",
    "c = ax.scatter(MA_alerts[\"month\"], MA_alerts[\"duration\"], marker=\"o\", linestyle=\"\", alpha = 0.6, s = 20, edgecolor = 'k')\n",
    "\n",
    "#labels\n",
    "ax.set_title('Duration, Month, and Island of Water Level Alerts', fontweight = 'bold')\n",
    "ax.set_ylim(-1, 65)\n",
    "ax.set_xlim(0.8, 12.2)\n",
    "ax.set_ylabel('Duration of Event (days)')\n",
    "ax.set_xlabel('Month of Event')\n",
    "\n",
    "\n",
    "ax.fill_between([4,5,6,7,8,9,10], -10, 80, color = 'tan', alpha = 0.3, zorder = 0) #dry season color\n",
    "ax.fill_between([0,1,2,3,4], -10, 80, color = 'cornflowerblue', alpha = 0.3, zorder = 0) #wet season color\n",
    "ax.fill_between([10, 11,12, 13], -10, 80, color = 'cornflowerblue', alpha = 0.3, zorder = 0) #wet season color\n",
    "ax.legend(['Kauai', 'Oahu', 'Maui', 'Dry Season', 'Wet Season'], fontsize = 7, loc= 'upper left') #legend\n",
    "plt.xticks([*range(1,13)])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot every alert through spatial timing of the alerts, duration, and location\n",
    "KA_alerts = alert_durations.loc[alert_durations['state'] == 1.0]\n",
    "# OA_alerts = alert_durations.loc[alert_durations['state'] == 2.0]\n",
    "# MA_alerts = alert_durations.loc[alert_durations['state'] == 3.0]\n",
    "\n",
    "x = KA_alerts.groupby('month')\n",
    "\n",
    "durations = []\n",
    "x = x.groups\n",
    "for key in x:\n",
    "    durations.append(x[key].tolist())\n",
    "\n",
    "#plot the points of alerts\n",
    "fig, ax = plt.subplots(figsize=(8,3))\n",
    "a = ax.boxplot(durations)\n",
    "# b = ax.scatter(OA_alerts[\"month\"], OA_alerts[\"duration\"], marker=\"o\", linestyle=\"\", alpha = 0.6, s = 20, edgecolor = 'k')\n",
    "# c = ax.scatter(MA_alerts[\"month\"], MA_alerts[\"duration\"], marker=\"o\", linestyle=\"\", alpha = 0.6, s = 20, edgecolor = 'k')\n",
    "\n",
    "#labels\n",
    "ax.set_title('Duration, Month, and Island of Water Level Alerts', fontweight = 'bold')\n",
    "ax.set_ylim(-1, 80)\n",
    "ax.set_xlim(0.7, 12.2)\n",
    "ax.set_ylabel('Duration of Event (days)')\n",
    "ax.set_xlabel('Month of Event')\n",
    "\n",
    "\n",
    "ax.fill_between([4,5,6,7,8,9,10], -10, 100, color = 'tan', alpha = 0.3, zorder = 0) #dry season color\n",
    "ax.fill_between([0,1,2,3,4], -10, 100, color = 'cornflowerblue', alpha = 0.3, zorder = 0) #wet season color\n",
    "ax.fill_between([10, 11,12, 13], -10, 100, color = 'cornflowerblue', alpha = 0.3, zorder = 0) #wet season color\n",
    "#ax.legend(['Kauai', 'Oahu', 'Maui', 'Dry Season', 'Wet Season'], fontsize = 7, loc= 'upper left') #legend\n",
    "plt.xticks([*range(1,13)])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = KA_alerts.groupby('month')\n",
    "\n",
    "durations = []\n",
    "x = x.groups\n",
    "for key in x:\n",
    "    durations.append(x[key].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Daily Slope Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the slopes using linear trends. Figure out the linear trend of the slope and the length of the slope in days. The next day must be a reversal for a new slope to be calculated. If the next day is at the same water level, continue calculating the same trend slope. Once the trend is reversed, the first and last point will be used to figure out the linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slopes(WL_data, date):\n",
    "   \n",
    "    all_slopes = []\n",
    "    start = []\n",
    "    end = []\n",
    "    all_total_n = []\n",
    "    flexible_range = 0 #flexible range on linear trend. right not set to be = to or greater/less than the previous value\n",
    "\n",
    "    i = 0\n",
    "    while i < (len(WL_data) - 1):\n",
    "        tally = [] #this is for appending WL data that follows a consistent up or down trend\n",
    "\n",
    "        if WL_data[i] != np.nan:\n",
    "            if WL_data[i + 1] != np.nan: #if the nxt value is nan forget about it\n",
    "                #in the case where the water level is going up\n",
    "                if WL_data[i + 1] >= (WL_data[i] + flexible_range):\n",
    "                    start.append(daily_dic[key]['date'][i])\n",
    "                    j = WL_data[i + 1] #next value is trendy\n",
    "                    starting_WL = WL_data[i] #start of the trend\n",
    "                    tally.append(starting_WL) #start tallyinhg up the WL data in trend\n",
    "                    while (j + flexible_range) >= starting_WL:\n",
    "                        if i <= (len(WL_data) -2): #so that the code doesnt break\n",
    "                            if WL_data[i + 1] != np.nan: #must not be a nan value\n",
    "                                j = WL_data[i+1] #next value follows the trend\n",
    "                                starting_WL = WL_data[i] #shift the start of the trend to the next value\n",
    "                                tally.append(j) # keep track of the data trend\n",
    "                                i+=1 # move to the next value in the WL data\n",
    "                            else:\n",
    "                                break #get out if the trend breaks\n",
    "                        else:\n",
    "                            break #if we are at the end of the data close the loop and ditch it\n",
    "                    tally.pop() #remove the last one because it is where the trend was broken\n",
    "                    total_n = len(tally) #total number of data samples in the trend\n",
    "                    all_total_n.append(total_n) #list of all total samples \n",
    "\n",
    "                    slope = (tally[-1] - tally[0]) / total_n #slope calculated by y2 - y1 / x2- x1 where x2 is the length of the samples and x1 starts at 0\n",
    "                    all_slopes.append(slope) # keep track of all of the slopes\n",
    "                    end.append(daily_dic[key]['date'][i])\n",
    "                    continue\n",
    "\n",
    "                #in the case where the water level is going down\n",
    "                if WL_data[i + 1] < (WL_data[i] + flexible_range):\n",
    "                    start.append(daily_dic[key]['date'][i])\n",
    "                    j = WL_data[i + 1]\n",
    "                    starting_WL = WL_data[i]\n",
    "                    tally.append(starting_WL)\n",
    "                    while (j - flexible_range) <= starting_WL:\n",
    "                        if i <= (len(WL_data) -2):\n",
    "                            if WL_data[i + 1] != np.nan:\n",
    "                                j = WL_data[i+1]\n",
    "                                starting_WL = WL_data[i]\n",
    "                                tally.append(j)\n",
    "                                i+=1\n",
    "                            else:\n",
    "                                break\n",
    "                        else:\n",
    "                            break\n",
    "                    tally.pop() #remove the last one because it is where the trend was broken\n",
    "                    total_n = len(tally)\n",
    "                    all_total_n.append(total_n)\n",
    "                    slope = (tally[-1] - tally[0]) / total_n\n",
    "                    all_slopes.append(slope)\n",
    "                    end.append(daily_dic[key]['date'][i])\n",
    "                    continue\n",
    "                    \n",
    "                \n",
    "                i +=1\n",
    "            else:\n",
    "                i +=1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    #create a dataframe of everything    \n",
    "    slopes_df = pd.DataFrame()\n",
    "    slopes_df['n_data'] = all_total_n\n",
    "    slopes_df['avg_WL_change_per_n'] = all_slopes\n",
    "    #-1 or 1 for the direction of the slopes\n",
    "    slopes_df['neg_pos'] = np.nan\n",
    "    slopes_df['start'] = start\n",
    "    slopes_df['end'] = end\n",
    "    for i in range(len(slopes_df)):\n",
    "        if slopes_df['avg_WL_change_per_n'][i] > 0:\n",
    "            slopes_df['neg_pos'][i] = 1\n",
    "        if slopes_df['avg_WL_change_per_n'][i] < 0:\n",
    "            slopes_df['neg_pos'][i] = -1\n",
    "\n",
    "    return slopes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function above to get all of the slopes for each reservoir water level data\n",
    "Reservoir_Slopes = {}\n",
    "for key in daily_dic:\n",
    "    Reservoir_Slopes[key] = get_slopes(daily_dic[key]['data'], daily_dic[key]['date'])\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the slope data as regular and as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outlier slopes from bad/suspicious data:\n",
    "for key in Reservoir_Slopes:\n",
    "    if key in ['EDD0411E', 'EDD05268', 'EDD0BF48', 'EDD06920', 'EDD08400']: #chosen based on suspicious looking water level graphs\n",
    "        data = Reservoir_Slopes[key]\n",
    "        STDs = 3 #three standard deviations\n",
    "        no_outliers = data[((data[['avg_WL_change_per_n']] - data[['avg_WL_change_per_n']].mean()) / data[['avg_WL_change_per_n']].std()).abs() < STDs]\n",
    "        Reservoir_Slopes[key]['avg_WL_change_per_n'] = no_outliers['avg_WL_change_per_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the slope points using the average linear trend and the length of the trend recorded\n",
    "#color the titles based on the hdyrologic response rating determined using the alert levels\n",
    "colors={'hazard' : 'red', 'problem' : 'orange', 'safe' : 'green', 'unknown' : 'black'} \n",
    "\n",
    "for key in Reservoir_Slopes:\n",
    "    print(key)\n",
    "    h_response = hydrologic_response[key]\n",
    "    c = colors[h_response]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    \n",
    "    #plot scatter\n",
    "    ax.scatter(Reservoir_Slopes[key]['n_data'], Reservoir_Slopes[key]['avg_WL_change_per_n'], s= 15, alpha = 0.65)\n",
    "    \n",
    "    if key != 'EDD01162' and key != 'EDD11398': #these reservoirs don't have enough data for means\n",
    "        #determine mean positive and mean negative trends\n",
    "        avgUPtrend = Reservoir_Slopes[key].groupby(['neg_pos'])['avg_WL_change_per_n'].mean().tolist()[1]\n",
    "        avgDOWNtrend = Reservoir_Slopes[key].groupby(['neg_pos'])['avg_WL_change_per_n'].mean().tolist()[0]\n",
    "        #plot the trends as horizontal line\n",
    "        plt.axhline(y = avgUPtrend, color = 'g', linestyle = '-')\n",
    "        plt.axhline(y = avgDOWNtrend, color = 'r', linestyle = '-')\n",
    "        \n",
    "    #labels\n",
    "    ax.set_title(key, color = c, fontweight = 'bold')\n",
    "    ax.set_xlabel('Length of Trend (days)')\n",
    "    ax.set_ylabel('Avg Change in WL During Trend')\n",
    "    \n",
    "    #grid to make it look nicer\n",
    "    a = plt.grid(alpha = 0.4, linestyle = '--', which =  'both')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMALIZE THE SLOPES:\n",
    "normalize the change in WL trends by the hydraulic height. Change it to percentages... How much does the WL change relative to the maximum hydraulic height. It will be normalized to percentages: If the hydraulic height is 10 ft and the trend is +1ft/day, then the % change is 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the slope trend data using the hydraulic heights\n",
    "hydr_height = pd.read_csv(os.path.join(\"..\", \"Data/Basin_dam_website_data\", '071723ForKoa_ResCordsData_editted2.csv'))\n",
    "for key in Reservoir_Slopes:\n",
    "    res_id = key.replace('WL_', '').replace('.csv', '') #get the Id without the file name .csv\n",
    "    if res_id in hydr_height.RES_ID.tolist():\n",
    "        reservoir_index = hydr_height.index[hydr_height['RES_ID']==res_id].tolist()[0]\n",
    "        hydraulic_height_val = hydr_height['Hydraulic_Height-ft'][reservoir_index]\n",
    "        Reservoir_Slopes[key]['normalized_change_WL'] = ( Reservoir_Slopes[key]['avg_WL_change_per_n'] / hydraulic_height_val ) * 100\n",
    "    else: \n",
    "        Reservoir_Slopes[key]['normalized_change_WL'] = np.nan\n",
    "\n",
    "#plot the normalized datas\n",
    "colors={'hazard' : 'red', 'problem' : 'orange', 'safe' : 'green', 'unknown' : 'gray'}\n",
    "for key in Reservoir_Slopes:\n",
    "    print(key)\n",
    "    h_response = hydrologic_response[key]\n",
    "    c = colors[h_response]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.scatter(Reservoir_Slopes[key]['n_data'], Reservoir_Slopes[key]['normalized_change_WL'], s= 15, alpha = 0.65)\n",
    "    \n",
    "    if key != 'EDD01162.csv' and key != 'EDD11398.csv':\n",
    "        avgUPtrend = Reservoir_Slopes[key].groupby(['neg_pos'])['normalized_change_WL'].mean().tolist()[1]\n",
    "        avgDOWNtrend = Reservoir_Slopes[key].groupby(['neg_pos'])['normalized_change_WL'].mean().tolist()[0]\n",
    "    \n",
    "        plt.axhline(y = avgUPtrend, color = 'g', linestyle = '-')\n",
    "        plt.axhline(y = avgDOWNtrend, color = 'r', linestyle = '-')\n",
    "    ax.set_title(key, color = c, fontweight = 'bold')\n",
    "    ax.set_xlabel('Length of Trend (days)')\n",
    "    ax.set_ylabel('% change in WL of the reservoir')\n",
    "    \n",
    "    \n",
    "    a = plt.grid(alpha = 0.4, linestyle = '--', which =  'both')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the Reservoirs using the Slope data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the categorization data using the top 5 maximum trends. Get the average up and down trneds. Then use the average up and down trend to figure out which way the WL slopes are skewed (positive or negative). To compare each reservoir to eachother, the data must be normalized again. The difference between the up and down trend is then normalized by the range of the up and down trend so that it is turned to percentages. This is important because if a reservoir has a 0.1 higher average up trend, this number is meaningless, but once normalized, it could mean that this reservoir has a 40% higher up trend compared to its down trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization = pd.DataFrame( columns = ['Reservoir', 'avg_max_inc_%WLchange' , 'avg_max_dec_%WLchange', 'mean_trend_weight_norm%', 'start_dates', 'end_dates'], index =[*range(0, len(Reservoir_Slopes))] )\n",
    "\n",
    "i = 0\n",
    "for key in Reservoir_Slopes:\n",
    "    if len(Reservoir_Slopes[key]) > 1 and np.isnan(Reservoir_Slopes[key].normalized_change_WL.mean()) == False: \n",
    "        #get the average % WL from the top 5 positive trends #get the average % WL from the top 5 negative trends\n",
    "        avg_max_negative_slope = Reservoir_Slopes[key].sort_values(by = 'normalized_change_WL')['normalized_change_WL'][0:5].mean()\n",
    "        avg_max_positive_slope = Reservoir_Slopes[key].sort_values(by = 'normalized_change_WL', ascending = False)['normalized_change_WL'][0:5].mean()\n",
    "        top5_startdates = Reservoir_Slopes[key].sort_values(by = 'normalized_change_WL')['start'][0:5]\n",
    "        top5_enddates = Reservoir_Slopes[key].sort_values(by = 'normalized_change_WL')['end'][0:5]\n",
    "        \n",
    "        #average slope trends\n",
    "        avgUPtrend = Reservoir_Slopes[key].groupby(['neg_pos'])['normalized_change_WL'].mean().tolist()[1]\n",
    "        avgDOWNtrend = Reservoir_Slopes[key].groupby(['neg_pos'])['normalized_change_WL'].mean().tolist()[0]\n",
    "\n",
    "        #get the difference between the mean positive trend and the mean negative trend and normalize it\n",
    "        mean_trend_weight = avgUPtrend - abs(avgDOWNtrend)\n",
    "        mean_trend_weight_normalized = (mean_trend_weight / (avgUPtrend + abs(avgDOWNtrend))) * 100\n",
    "        slope_categorization['start_dates'][i] = top5_startdates\n",
    "        slope_categorization['end_dates'][i] = top5_enddates\n",
    "        slope_categorization['Reservoir'][i] = key\n",
    "        slope_categorization['avg_max_inc_%WLchange'][i] = avg_max_positive_slope\n",
    "        slope_categorization['avg_max_dec_%WLchange'][i] = avg_max_negative_slope\n",
    "        slope_categorization['mean_trend_weight_norm%'][i] = mean_trend_weight_normalized\n",
    "\n",
    "    else: \n",
    "        slope_categorization['start_dates'][i] = top5_startdates\n",
    "        slope_categorization['end_dates'][i] = top5_enddates\n",
    "        slope_categorization['Reservoir'][i] = key\n",
    "        slope_categorization['avg_max_inc_%WLchange'][i] = np.nan\n",
    "        slope_categorization['avg_max_dec_%WLchange'][i] = np.nan\n",
    "        slope_categorization['mean_trend_weight_norm%'][i] = np.nan\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "#get the dlnrid\n",
    "dlnrid= []\n",
    "Station_Meta = Station_Meta.set_index('id')\n",
    "\n",
    "for ID in slope_categorization.Reservoir:\n",
    "    dlnrid.append(Station_Meta.dlnrid[ID])\n",
    "\n",
    "slope_categorization['dlnrid'] = dlnrid\n",
    "Station_Meta = Station_Meta.reset_index()\n",
    "\n",
    "#get the hydrologic response from \n",
    "hydr_resp = []\n",
    "for ID in slope_categorization.Reservoir:\n",
    "    hydr_resp.append(hydrologic_response[ID])\n",
    "\n",
    "slope_categorization['hydrologic_response'] = hydr_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization.to_csv('slope_categorization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization = slope_categorization.reset_index()\n",
    "slope_categorization = slope_categorization.set_index('dlnrid')\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "data_info = data_info.reset_index()\n",
    "data_info = data_info.set_index('dlnrid')\n",
    "\n",
    "for i in data_info.index:\n",
    "    dlnrid = i\n",
    "    st = slope_categorization.start_dates[dlnrid].tolist()\n",
    "    en = slope_categorization.end_dates[dlnrid].tolist()\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.2, linestyle = '--', color = 'blue', lw = 2)\n",
    "    ax.hlines(y= dlnrid, xmin = st[0], xmax = en[0], colors = 'darkred', lw = 10) \n",
    "    ax.hlines(y= dlnrid, xmin = st[1], xmax = en[1], colors = 'darkred', lw = 10) \n",
    "    ax.hlines(y= dlnrid, xmin = st[2], xmax = en[2], colors = 'darkred', lw = 10) \n",
    "    ax.hlines(y= dlnrid, xmin = st[3], xmax = en[3], colors = 'darkred', lw = 10) \n",
    "    ax.hlines(y= dlnrid, xmin = st[4], xmax = en[4], colors = 'darkred', lw = 10) \n",
    "    \n",
    "\n",
    "#title and axis labels\n",
    "ax.set_title('Top 5 Events with the Highest Water Level Increases', fontweight = 'bold', fontsize = 15)\n",
    "ax.set_ylabel('DLNR Reservoir ID', weight = 'bold', fontsize = 14)\n",
    "ax.set_xlabel('Date', weight = 'bold', fontsize = 14)\n",
    "\n",
    "\n",
    "\n",
    "#plot the dash lines \n",
    "a = plt.grid(alpha = 0.25, linestyle = '--', which =  'both', axis = 'x')\n",
    "b = plt.grid(alpha = 0.7, linestyle = '--', which =  'both', axis = 'y')\n",
    "#b = plt.grid(alpha = 0.14, linestyle = '--', color = 'r', axis = 'y', lw = '2')\n",
    "\n",
    "slope_categorization = slope_categorization.reset_index()\n",
    "data_info = data_info.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Differences in WL Increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = data_info.reset_index()\n",
    "slope_categorization = slope_categorization.reset_index()\n",
    "slope_categorization = slope_categorization.set_index('dlnrid')\n",
    "data_info = data_info.set_index('dlnrid')\n",
    "\n",
    "all_starts = []\n",
    "all_ends = []\n",
    "all_seasons = []\n",
    "all_dlnrid = []\n",
    "\n",
    "for i in data_info.index:\n",
    "    dlnrid = i\n",
    "    st = slope_categorization.start_dates[dlnrid].tolist()\n",
    "    en = slope_categorization.end_dates[dlnrid].tolist()\n",
    "\n",
    "    months = []\n",
    "    for i in st:\n",
    "        if i.month >= 11:\n",
    "            month = 'rainy'\n",
    "        elif i.month < 4:\n",
    "            month = 'rainy'\n",
    "        else:\n",
    "            month = 'dry'\n",
    "        months.append(month)\n",
    "   \n",
    "    all_starts.append(st)\n",
    "    all_seasons.append(months)\n",
    "    all_ends.append(en)\n",
    "    all_dlnrid.append(dlnrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {'start' : all_starts, 'end': all_ends, 'seasons' : all_seasons, 'dlnrid' : all_dlnrid}\n",
    "df = pd.DataFrame(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainy = 0\n",
    "dry = 0 \n",
    "\n",
    "\n",
    "for i in df.index:\n",
    "    for x in df.seasons[i]:\n",
    "        if x == 'rainy':\n",
    "            rainy+= 1\n",
    "        if x == 'dry':\n",
    "            dry += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "#bars\n",
    "rects = ax.bar(['Wet', 'Dry'], [rainy, dry], color = ['teal', 'tan'], edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.bar\n",
    "\n",
    "#labels\n",
    "plt.title('Seasonal Variations of \\n INcreasing Water Level Events', fontweight  = 'bold')\n",
    "ax.set_xlabel('Season')\n",
    "ax.set_ylabel('Total Number of Events')\n",
    "\n",
    "#margins and stuff\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('seasonal_alert_tally.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alert Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = data_info.reset_index()\n",
    "slope_categorization = slope_categorization.reset_index()\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "slope_categorization = slope_categorization.set_index('dlnrid')\n",
    "data_info = data_info.set_index('dlnrid')\n",
    "\n",
    "#plot the timelines by state...\n",
    "groups = KA_alerts.groupby(['dlnrid'])\n",
    "for name, group in groups:\n",
    "    ax.hlines(y= group['dlnrid'], xmin = group[\"alert_start\"], xmax = group[\"alert_end\"], colors = 'r', lw = 10)\n",
    "    #plot start and end of data\n",
    "    dlnrid = group['dlnrid'].reset_index(drop = True)[0]\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "groups = OA_alerts.groupby(['dlnrid'])\n",
    "for name, group in groups:\n",
    "    ax.hlines(y= group['dlnrid'], xmin = group[\"alert_start\"], xmax = group[\"alert_end\"], colors = 'r', lw = 10)\n",
    "        #plot start and end of data\n",
    "    dlnrid = group['dlnrid'].reset_index(drop = True)[0]\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "groups = MA_alerts.groupby(['dlnrid'])\n",
    "for name, group in groups:\n",
    "    ax.hlines(y= group['dlnrid'], xmin = group[\"alert_start\"], xmax = group[\"alert_end\"], colors = 'r', lw = 10)\n",
    "    #plot start and end of data\n",
    "    dlnrid = group['dlnrid'].reset_index(drop = True)[0]\n",
    "    ax.hlines(y= dlnrid, xmin = data_info.start_date[dlnrid], xmax = data_info.end_date[dlnrid], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "\n",
    "\n",
    "reservoirs_w_alerts = []    \n",
    "#get a list of all reservoirs with alerts\n",
    "reservoirs_w_alerts.append(KA_alerts.dlnrid.unique().tolist())\n",
    "reservoirs_w_alerts.append(OA_alerts.dlnrid.unique().tolist())\n",
    "reservoirs_w_alerts.append(MA_alerts.dlnrid.unique().tolist())\n",
    "reservoirs_w_alerts = list((itertools.chain.from_iterable(reservoirs_w_alerts)))\n",
    "\n",
    "\n",
    "\n",
    "#plot timeseries of non alerts\n",
    "result = result.set_index('dlnrid')\n",
    "for i in result.index:\n",
    "    if i in reservoirs_w_alerts:\n",
    "        continue\n",
    "    else: \n",
    "        ax.hlines(y= i, xmin = result.start_date[i], xmax = result.end_date[i], alpha = 0.3, linestyle = '--', color = 'blue', lw = 2)\n",
    "result = result.reset_index() \n",
    "\n",
    "\n",
    "#### plot  the x axis date format stuff\n",
    "ax.xaxis.set_major_locator(matplotlib.dates.YearLocator())\n",
    "ax.xaxis.set_minor_locator(matplotlib.dates.MonthLocator((4,7,10)))\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\"\\n%Y\"))\n",
    "ax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\"%b\"))\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\", fontsize = 10)\n",
    "plt.gca().set_xbound(pd.to_datetime('01-01-2018'), pd.to_datetime('07-01-2024')) # Changes x-axis range\n",
    "\n",
    "\n",
    "#title and axis labels\n",
    "ax.set_title('Reservoir Data and Alerts Timeline', fontweight = 'bold', fontsize = 19)\n",
    "ax.set_ylabel('DLNR Reservoir ID', weight = 'bold', fontsize = 14)\n",
    "ax.set_xlabel('Date', weight = 'bold', fontsize = 14)\n",
    "\n",
    "\n",
    "\n",
    "#plot the dash lines \n",
    "a = plt.grid(alpha = 0.45, linestyle = '--', which =  'both', axis = 'x')\n",
    "b = plt.grid(alpha = 0.2, linestyle = '--', which =  'both', axis = 'y')\n",
    "#b = plt.grid(alpha = 0.14, linestyle = '--', color = 'r', axis = 'y', lw = '2')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('WL_alert_timeline.png')\n",
    "\n",
    "data_info = data_info.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantify the balance of the reservoir system: if the up trend is more than 5% higher than the down trend, then this reservoir tends to have higher inflows compared to its outflows on average.\n",
    "\n",
    "Quantify the risk of WL increas: if the maximum WL increase trend is greater than 2% of the total hydraulic height per day, then this is risky cuz the reservoir could fill up very fast and overturn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantify the balance of the reservoir system\n",
    "slope_categorization['operational_stability'] = np.nan\n",
    "for i in range(len(slope_categorization)):\n",
    "    if slope_categorization['mean_trend_weight_norm%'][i] < 5:\n",
    "        slope_categorization['operational_stability'][i] = 'NEUTRAL'\n",
    "    else: \n",
    "        slope_categorization['operational_stability'][i] = 'SKEWED'\n",
    "    if np.isnan(slope_categorization['mean_trend_weight_norm%'][i]) == True:\n",
    "        slope_categorization['operational_stability'][i] = np.nan\n",
    "        \n",
    "#quantify the risk of WL increase\n",
    "slope_categorization['WL_pos_change_potential'] = np.nan\n",
    "for i in range(len(slope_categorization)):\n",
    "    if slope_categorization['avg_max_inc_%WLchange'][i] > 2:\n",
    "        slope_categorization['WL_pos_change_potential'][i] = 'HIGH'\n",
    "    else: \n",
    "        slope_categorization['WL_pos_change_potential'][i] = 'LOW'\n",
    "    if np.isnan(slope_categorization['avg_max_inc_%WLchange'][i]) == True:\n",
    "        slope_categorization['WL_pos_change_potential'][i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization = slope_categorization.sort_values(by =['avg_max_inc_%WLchange'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "#bars\n",
    "rects = ax.bar(slope_categorization.dlnrid, slope_categorization['avg_max_inc_%WLchange'], edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.bar\n",
    "\n",
    "#labels\n",
    "plt.title('Average Daily Percent Change in Water Level heights (relative to hydraulic height) \\n from the 5 Highest Increasing Trends ', fontweight  = 'bold')\n",
    "ax.set_xlabel('DLNR Reservoir id')\n",
    "ax.set_ylabel('Daily Increase in Water Level (%)')\n",
    "plt.axhline(y = 3, color = 'r', linestyle = '-')\n",
    "\n",
    "#margins and stuff\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', which='major', axis='y', linestyle='dashed', alpha = 0.25)\n",
    "plt.savefig('trends.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_categorization['hydr_resp_SLOPE_safety_rating'] = np.nan\n",
    "for i in range(len(slope_categorization)):\n",
    "    \n",
    "    #if the operations are nuetral and the WL change i low, the reservoir is level 1 safe\n",
    "    if slope_categorization['operational_stability'][i] == 'NEUTRAL' and slope_categorization['WL_pos_change_potential'][i] == 'LOW' :\n",
    "        slope_categorization['hydr_resp_SLOPE_safety_rating'][i] = 1\n",
    "        \n",
    "    #if the operations are skewed and the WL change is low, the reservoir is level 2 problematic\n",
    "    elif slope_categorization['operational_stability'][i] == 'SKEWED' and slope_categorization['WL_pos_change_potential'][i] == 'LOW' :\n",
    "        slope_categorization['hydr_resp_SLOPE_safety_rating'][i] = 2\n",
    "    \n",
    "    #if the operations are nuetral and the WL change is high, the reservoir is level 3 problematic\n",
    "    elif slope_categorization['operational_stability'][i] == 'NEUTRAL' and slope_categorization['WL_pos_change_potential'][i] == 'HIGH' :\n",
    "        slope_categorization['hydr_resp_SLOPE_safety_rating'][i] = 3\n",
    "        \n",
    "    #if the operations are skewed and the WL change is high, the reservoir is level 4 hazardous\n",
    "    elif slope_categorization['operational_stability'][i] == 'SKEWED' and slope_categorization['WL_pos_change_potential'][i] == 'HIGH' :\n",
    "        slope_categorization['hydr_resp_SLOPE_safety_rating'][i] = 4\n",
    "        \n",
    "        \n",
    "    if np.isnan(slope_categorization['avg_max_inc_%WLchange'][i]) == True:\n",
    "        slope_categorization['hydr_resp_SLOPE_safety_rating'][i] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#compare the ratings to the original ratings....\n",
    "slope_categorization['Original_Hydrologic_response_rating'] = np.nan\n",
    "\n",
    "for key in hydrologic_response:\n",
    "    index = slope_categorization.index[slope_categorization['Reservoir']==key].tolist()[0]\n",
    "    slope_categorization['Original_Hydrologic_response_rating'][index] = hydrologic_response[key]\n",
    "    \n",
    "#slope_categorization.to_csv('')\n",
    "\n",
    "x = slope_categorization.groupby('hydr_resp_SLOPE_safety_rating')\n",
    "Reservoir_Slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all of the categorized trends for ratings 1-4 and make a plot to visualize the differences between the 4 categorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe = x.get_group(1)['Reservoir'].tolist()\n",
    "fig, ax = plt.subplots(4, figsize=(8,15))\n",
    "Descriptions = ['Balanced trends, Low positive trends', 'Imbalanced trends, Low positive trends', 'Balanced trends, High positive trends', 'Imbalanced trends, High positive trends']\n",
    "for i in range(1,5):\n",
    "    hazard_rating = i\n",
    "    reservoir_list = x.get_group(i)['Reservoir'].tolist()\n",
    "    for res in reservoir_list:\n",
    "        ax[i-1].scatter(Reservoir_Slopes[res].n_data, Reservoir_Slopes[res].normalized_change_WL, s=2, c = 'b')\n",
    "        ax[i-1].set_title('\\n'+'Hazard Rating : ' + str(i) +'\\n' + Descriptions[i-1], fontweight = 'bold')\n",
    "        ax[i-1].set_xlabel('Length of Trend (days)', fontsize = 7)\n",
    "        ax[i-1].set_ylabel('% change in WL', fontsize = 7)\n",
    "        ax[i-1].set_xlim(0, 40)\n",
    "        ax[i-1].axhline(0, zorder = 0, linestyle = '--', c = 'red', alpha = 0.1)\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull out the top 5 WL trend increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "Reservoir_Slopes\n",
    "ids = []\n",
    "Max_WL_trend = []\n",
    "Max_WL_increase = []\n",
    "\n",
    "for key in Reservoir_Slopes:\n",
    "    ids.append(key)\n",
    "    df= Reservoir_Slopes[key]\n",
    "    df = df.drop('normalized_change_WL', axis=1)\n",
    "    df = df.dropna(subset=['avg_WL_change_per_n']) \n",
    "    df['total_change'] = df['n_data'] * df['avg_WL_change_per_n']\n",
    "    max_increase = statistics.mean(df.sort_values('total_change')[-6:-1]['total_change'].tolist())\n",
    "    \n",
    "    top5_WL_daily_trend = df.sort_values('avg_WL_change_per_n')[-6:-1]['avg_WL_change_per_n'].tolist()\n",
    "    num_days_trend = df.sort_values('avg_WL_change_per_n')[-6:-1]['n_data'].tolist()\n",
    "#     total_WL_increase = []\n",
    "#     for i in range(len(top5_WL_daily_trend)):\n",
    "#         total_WL_increase.append(top5_WL_daily_trend[i] * num_days_trend[i])\n",
    "    Max_WL_trend.append(top5_WL_daily_trend)\n",
    "    Max_WL_increase.append(max_increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= Reservoir_Slopes[key]\n",
    "df = df.drop('normalized_change_WL', axis=1)\n",
    "df = df.dropna(subset=['avg_WL_change_per_n']) \n",
    "df.sort_values('avg_WL_change_per_n')[-6:-1]['avg_WL_change_per_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_WL_increase\n",
    "\n",
    "dict = {'dlnrid': ids, 'max_WL_increase': Max_WL_increase} \n",
    "   \n",
    "df = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'dlnrid': ids, 'top5_maxWL': Max_WL_trend} \n",
    "top5 = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "#get the dlnrid\n",
    "dlnrid= []\n",
    "Station_Meta = Station_Meta.set_index('id')\n",
    "\n",
    "for ID in slope_categorization.Reservoir:\n",
    "    dlnrid.append(Station_Meta.dlnrid[ID])\n",
    "\n",
    "df['dlnrid'] = dlnrid\n",
    "Station_Meta = Station_Meta.reset_index()\n",
    "\n",
    "#get the hydrologic response from \n",
    "hydr_resp = []\n",
    "for ID in slope_categorization.Reservoir:\n",
    "    hydr_resp.append(hydrologic_response[ID])\n",
    "\n",
    "df['hydrologic_response'] = hydr_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('max_WL_increase_percent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Between Mean and Alert Level ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Station_Meta = Station_Meta.set_index('dlnrid')\n",
    "stats_df = stats_df.set_index('dlrnid')\n",
    "dlnrids = []\n",
    "alert_mean_dif = [] \n",
    "for i in stats_df.index:\n",
    "\n",
    "    dlnrids.append(i)\n",
    "    \n",
    "    mean = stats_df['mean'][i]\n",
    "    alert = Station_Meta.level_alert_on[i]\n",
    "    difference = abs(alert - mean)\n",
    "    \n",
    "    alert_mean_dif.append(difference)\n",
    "    \n",
    "d = {'dlrnid': dlnrids, 'alert_mean_difference': alert_mean_dif}\n",
    "alert_mean_diff_df = pd.DataFrame(data=d)\n",
    "alert_mean_diff_df = alert_mean_diff_df.sort_values(by = ['alert_mean_difference']) \n",
    "alert_mean_diff_df = alert_mean_diff_df.dropna(how='any')\n",
    "\n",
    "Station_Meta = Station_Meta.reset_index('dlnrid')\n",
    "stats_df = stats_df.reset_index('dlrnid')\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = result\n",
    "\n",
    "#Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "\n",
    "\n",
    "#bar graphs\n",
    "rects = ax.bar(alert_mean_diff_df.dlrnid, alert_mean_diff_df.alert_mean_difference, edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.yticks(np.arange(0, 300, 25))\n",
    "\n",
    "\n",
    "\n",
    "#the one year line\n",
    "#a = ax.axhline(y = 1, color = 'r', linestyle = '-') \n",
    "\n",
    "#labels and margins\n",
    "plt.title('Water Level (ft) Difference Between Mean and Alert Level ON', fontweight  = 'bold')\n",
    "ax.set_xlabel('DLNR Reservoir id')\n",
    "ax.set_ylabel('Alert ON and Mean Difference (ft)')\n",
    "\n",
    "\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "plt.tight_layout()\n",
    "plt.grid(color='black', which='major', axis='y', linestyle='dashed', alpha = 0.25)\n",
    "\n",
    "plt.savefig('alert_mean_diff.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Stats¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLE CODE\n",
    "\n",
    "\n",
    "# for file in WL_alerts:\n",
    "#     WL_alerts[file]['event_start'] = pd.to_datetime(WL_alerts[file].event_start)\n",
    "#     WL_alerts[file]['event_end'] = pd.to_datetime(WL_alerts[file].event_end)\n",
    "#     alert_months = WL_alerts[file].event_start.dt.month\n",
    "\n",
    "#     for month in range(len(alert_months)):\n",
    "#         alert_start_tally.append(WL_alerts[file]['event_start'][month])\n",
    "#         alert_end_tally.append(WL_alerts[file]['event_end'][month])\n",
    "#         duration_tally.append(WL_alerts[file]['duration'][month])\n",
    "#         reservoir.append(file)\n",
    "#         if month > 3 and month < 11:\n",
    "#             dry += 1\n",
    "#         else:\n",
    "#             wet += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dictionary for holding month column\n",
    "seasonal_daily_dic = daily_dic\n",
    "\n",
    "\n",
    "for key in seasonal_daily_dic:\n",
    "    seasonal_daily_dic[key] = seasonal_daily_dic[key].reset_index()\n",
    "    seasonal_daily_dic[key]['month'] = np.nan\n",
    "    for i in range(len(seasonal_daily_dic[key])):\n",
    "        month = seasonal_daily_dic[key].date[i].month\n",
    "        seasonal_daily_dic[key]['month'][i] = month  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_id = [] \n",
    "wet_mean = []\n",
    "dry_mean = []\n",
    "annual = []\n",
    "stdev = []\n",
    "\n",
    "\n",
    "for key in seasonal_daily_dic: \n",
    "    seasonal_id.append(key)\n",
    "    \n",
    "    df = seasonal_daily_dic[key]\n",
    "\n",
    "    df['season'] = np.nan\n",
    "    for i in range(len(df)):\n",
    "        if df.month[i] >= 11 : \n",
    "            df.season[i] = 'wet'\n",
    "        elif df.month[i] <= 4 : \n",
    "            df.season[i] = 'wet' \n",
    "        else: \n",
    "            df.season[i] = 'dry' \n",
    "\n",
    "\n",
    "    annual.append(df.data.mean())\n",
    "    stdev.append(df.data.std())\n",
    "    dry = df.loc[(df['season'] == 'dry')]\n",
    "    dry_mean.append(dry.data.mean())\n",
    "\n",
    "    wet = df.loc[(df['season'] == 'wet')]\n",
    "    wet_mean.append(wet.data.mean())\n",
    "    \n",
    "# dictionary of lists \n",
    "seasonal_stats = pd.DataFrame({'UHSLC_USGS_id': seasonal_id, 'dry_mean': dry_mean, 'wet_mean': wet_mean , 'annual' : annual, 'stdev' : stdev} )\n",
    "   \n",
    "#other stats\n",
    "seasonal_stats['seasonal_diff_in_std'] = (seasonal_stats['wet_mean'] - seasonal_stats['dry_mean']) / seasonal_stats['stdev']\n",
    "seasonal_stats['wet_season_difference'] = seasonal_stats['wet_mean'] - seasonal_stats['annual']\n",
    "seasonal_stats['dry_season_difference'] = seasonal_stats['dry_mean'] - seasonal_stats['annual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dlnrid \n",
    "seasonal_stats = seasonal_stats.set_index('UHSLC_USGS_id')\n",
    "seasonal_stats['dlnrid'] = np.nan\n",
    "Station_Meta = Station_Meta.set_index('id')\n",
    "for i in Station_Meta.index: \n",
    "    dlnrid = Station_Meta['dlnrid'][i]\n",
    "    seasonal_stats['dlnrid'][i] = dlnrid\n",
    "    \n",
    "seasonal_stats = seasonal_stats.reset_index()\n",
    "Station_Meta = Station_Meta.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_stats.to_csv('seasonal_stats.csv')\n",
    "seasonal_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_stats = seasonal_stats.sort_values('seasonal_diff_in_std')\n",
    "\n",
    "# #Create Barplot:\n",
    "fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#bar graphs\n",
    "rects = ax.bar(seasonal_stats.dlnrid, seasonal_stats.seasonal_diff_in_std, edgecolor='black', linewidth=1.2)\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "plt.yticks(np.arange(-1.5, 2, 0.5))\n",
    "\n",
    "\n",
    "#labels and margins\n",
    "plt.title('Variation in Seasonal Average Water Level \\n Compared to the Annual Average and Standard Deviation', fontweight  = 'bold')\n",
    "ax.set_xlabel('DLNR Reservoir id')\n",
    "ax.set_ylabel('Number of Standard Deviations \\nthe Seasonal Water Level Average \\n Differs from the Annual Average')\n",
    "\n",
    "ax.set_ylim([-1.5, 1.5])\n",
    "ax.margins(x=0.008, y=0.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.grid(color='black', which='major', axis='y', linestyle='dashed', alpha = 0.25, zorder = 0)\n",
    "ax.grid(alpha = 1, linestyle = '--', which =  'both', axis = 'x', zorder = 0)\n",
    "ax.grid(alpha = 0.1, linestyle = '--', which =  'both', axis = 'y', zorder = 0)\n",
    "\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "\n",
    "plt.savefig('seasonal_variation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAINDATA\n",
    "using statewide data download from HCDP, its actually a total pain. The data formattng is strange, and station metadata is inconsistent. Oh well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat rain data \n",
    "\n",
    "Datadic = {}\n",
    "metadata_dic = {}\n",
    "\n",
    "\n",
    "raindatapath = \"Data/External_data/Precip/2018-2022_station_data\"\n",
    "\n",
    "for year in os.listdir(os.path.join(\"..\", raindatapath)):\n",
    "    for month in os.listdir(os.path.join(\"..\", raindatapath, year)):\n",
    "        file = os.listdir(os.path.join(\"..\", raindatapath, year, month))\n",
    "        \n",
    "        # Process into a better format where columns are individual stations and y axis is date\n",
    "        data = pd.read_csv(os.path.join(\"..\", raindatapath, year, month, file[0]))\n",
    "        \n",
    "        # Extract a separaate dataframe of metadata \n",
    "        data_metaonly = data[['SKN', 'Station.Name', 'Observer', 'Network', 'Island', 'ELEV.m.',\n",
    "       'LAT', 'LON', 'NCEI.id', 'NWS.id', 'NESDIS.id', 'SCAN.id', 'SMART_NODE_RF.id']]\n",
    "        metadata_dic[file[0]]  = data_metaonly\n",
    "        \n",
    "        \n",
    "        data.drop([ 'Station.Name', 'Observer', 'Network', 'Island', 'ELEV.m.',\n",
    "               'LAT', 'LON', 'NCEI.id', 'NWS.id', 'NESDIS.id', 'SCAN.id',\n",
    "               'SMART_NODE_RF.id'], axis=1, inplace=True)\n",
    "\n",
    "        data = data.set_index(\"SKN\", drop=True)\n",
    "        data = data.transpose()\n",
    "\n",
    "        data = data.reset_index()\n",
    "        data['index'] = data['index'].astype(str)\n",
    "\n",
    "        data['index'] = data['index'].apply(lambda x: x[1:])  # Pull out the X\n",
    "        data['index'] = pd.to_datetime(data['index'], yearfirst=True)  # .dt.strftime(\"%Y-%m-%d\")\n",
    "        data.set_index(\"index\", inplace=True)\n",
    "        data.columns.name = None   # drop the wierd double axis label \n",
    "        \n",
    "        Datadic[file[0]]  = data \n",
    "        \n",
    "# Produce the rainfall dataframe \n",
    "RainData = pd.concat(Datadic.values(), sort=True)  \n",
    "# Produce the metadataframe \n",
    "RainData_Meta = pd.concat(metadata_dic.values(), sort=True) \n",
    "\n",
    "# SO herees a problem, NO field in the metadata is unique, meaning there is some f-up in the station IDentifiers.  Creating a *hoprefully* unique ID with a couple fields\n",
    "RainData_Meta['UID'] = RainData_Meta['LAT'].map('{:.6f}'.format).astype(str)+\"_\"+RainData_Meta['LON'].map('{:.6f}'.format).astype(str)\n",
    "RainData_Meta.drop_duplicates(subset='UID', inplace=True)     # Hoping to limit to to non-duplicate stations    \n",
    "\n",
    "# genearate near table outside in Arc \n",
    "RainData_Meta[\"SKN\"] = RainData_Meta[\"SKN\"].astype(str)+\"_R\"   # First make Arc read it as a string \n",
    "RainData_Meta.to_csv('RainData_Meta_v2.csv')\n",
    "# NOW Open in Arc and do a \"Point Distance\" to generate a near table for each of the reservoirs and each rain location point\n",
    "# Then need to filter pertinant fields and so a table join based on the table FIDs\n",
    "RainData_Meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the nearest rainfall stations to each of the gauges\n",
    "\n",
    "n = 5   # The number of rain stations to use\n",
    "near_table = pd.read_csv(os.path.join(\"..\", \"Data/External_data/Precip/2018-2022_station_data_GIS/Reservoirs_Precip_Near_table_V2.csv\"))\n",
    "\n",
    "neartable_N = pd.DataFrame(columns=near_table.columns)\n",
    "\n",
    "for i in near_table['id'].unique():\n",
    "    temptable = near_table[near_table['id'] == i]  # For each of the reservoits\n",
    "    \n",
    "    temptable = temptable.sort_values('DISTANCE')  # find the closest stations\n",
    "    temptable = temptable.iloc[0:n]             # select only the n closest stations \n",
    "    neartable_N = neartable_N.append(temptable)               # Create dataframe of only the n closest stations \n",
    "    \n",
    "neartable_N.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create plots of reservoir level and rainfall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "smalldic = {key: daily_dic[key] for key in ['EDD00214', 'EDD00CC6', 'EDD01FB0', 'EDD024F8']}   # For testing \n",
    "\n",
    "for i in daily_dic.keys():  #daily_dic.keys(): \n",
    "    print(i)\n",
    "\n",
    "    stat_id = i[3:-4]\n",
    "    isla = Station_Meta['dlnrid'][Station_Meta['id'] == stat_id].values[0].split('-')[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    daily_dic[i].plot(y='data', ax=ax, marker=\".\", color='k', alpha=0.5, label=\"WL_(ft)\")\n",
    "    \n",
    "    # Add the lever alert lines\n",
    "    Level_alert_on = Station_Meta['level_alert_on'][Station_Meta['id'] == stat_id].values[0]\n",
    "    plt.axhline(y=Level_alert_on, color='r', linestyle='-', alpha = 0.5)   \n",
    "    Level_alert_off = Station_Meta['level_alert_off'][Station_Meta['id'] == stat_id].values[0]\n",
    "    plt.axhline(y=Level_alert_off, color='g', linestyle='-', alpha = 0.5)\n",
    "    \n",
    "    \n",
    "    # Add rainfall \n",
    "    \n",
    "    # Create average rainfall from nearest N stations \n",
    "    ax2=ax.twinx()\n",
    "    nearest_SKN_table = neartable_N[neartable_N['id'] == stat_id]     # Reference the nearest SKN stations for rainfall \n",
    "    # list nearest 5 skns\n",
    "    nearest5skns = list(nearest_SKN_table['SKN'])                      # Pull out the SKN vales from the dataframe \n",
    "    nearest5skns = list(map(lambda x: x.split(\"_\")[0], nearest5skns))  # Quick list function map to pull of the \"_R\" from each SKN\n",
    "    nearest5skns = list(map(lambda x: float(x), nearest5skns))  # Quick list function map to turn values back into floats\n",
    "    rain_datatoplot_5 = RainData[nearest5skns]     # make it a single dataframe \n",
    "    rain_datatoplot_5_mean = rain_datatoplot_5.mean(axis=1)    # calculate the average rainfall over all N stations \n",
    "    \n",
    "    \n",
    "    # Find the nearest SKN \n",
    "    nearest_SKN_table_min = nearest_SKN_table[nearest_SKN_table['DISTANCE'] == nearest_SKN_table['DISTANCE'].min()]\n",
    "    nearest_SKN = nearest_SKN_table_min['SKN'].values[0]\n",
    "    nearest_SKN_numeric = float(nearest_SKN.split(\"_\")[0])\n",
    "    rain_datatoplot = RainData[nearest_SKN_numeric]\n",
    "\n",
    "    \n",
    "    # FOR JUST THE NEAREST STATION \n",
    "    #ax2.plot(rain_datatoplot, color='b', alpha=0.2)\n",
    "    \n",
    "    \n",
    "    # FOR THE NEAREST N STATIONS\n",
    "    ax2.plot(rain_datatoplot_5_mean, color='b', alpha=0.2)    \n",
    "       \n",
    "    plt.title(stat_id +\" \"+ \"{}\".format(Station_Meta['name'][Station_Meta['id'] == stat_id].values[0])+\"--\"+isla)\n",
    "    ax2.set_ylabel('Rainfall (mm)', color='b')\n",
    "    ax.set_ylabel('WL-reservoir (ft)', color='k')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    MergeFrame = daily_dic[i].merge(rain_datatoplot_5_mean.to_frame(), left_index=True, right_index=True, how='inner')\n",
    "    MergeFrame.to_csv(os.path.join(\"..\", \"Brian_testing/Merged_DataFrames\", \"Merged_{}\".format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
