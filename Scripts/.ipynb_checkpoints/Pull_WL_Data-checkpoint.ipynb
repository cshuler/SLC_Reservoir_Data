{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "13b682f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c4b70",
   "metadata": {},
   "source": [
    "### Pulling data from the SLC API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0685da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDD00214\n",
      "EDD00CC6\n",
      "EDD01162\n",
      "EDD024F8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgorb\\AppData\\Local\\Temp\\ipykernel_21684\\884768840.py:19: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('https://uhslc.soest.hawaii.edu/reservoir/'  + ID  +  '-full.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDD02A2A\n",
      "EDD0378E\n",
      "EDD0395C\n",
      "EDD0411E\n",
      "EDD04FCC\n",
      "EDD05268\n",
      "EDD05CBA\n",
      "EDD067F2\n",
      "EDD06920\n",
      "EDD07484\n",
      "EDD07A56\n",
      "EDD08400\n",
      "EDD08AD2\n",
      "EDD099A4\n",
      "EDD0A2EC\n",
      "EDD0AC3E\n",
      "EDD0B19A\n",
      "EDD0BF48\n",
      "EDD0C70A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgorb\\AppData\\Local\\Temp\\ipykernel_21684\\884768840.py:19: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('https://uhslc.soest.hawaii.edu/reservoir/'  + ID  +  '-full.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDD0C9D8\n",
      "EDD0D47C\n",
      "EDD0DAAE\n",
      "EDD11398\n"
     ]
    }
   ],
   "source": [
    "# First Set up the loacation of where to put the data\n",
    "today = datetime.today().strftime('%Y-%m-%d')            # new data for each date so record the date\n",
    "os.makedirs(os.path.join(\"..\", \"Data\", \"WL_Data{}\".format(today), \"UHSLC\"), exist_ok=True) # create a new folder for todays date \n",
    "\n",
    "# Definte the folder to save files into \n",
    "Datapath = os.path.join(\"..\", \"Data\", \"WL_Data{}\".format(today), \"UHSLC\")\n",
    "\n",
    "# then Grab the station metadata for each of the reservoirs from SLC website\n",
    "url = 'https://uhslc.soest.hawaii.edu/reservoir/stations.geojson'\n",
    "meta = pd.json_normalize(requests.get(url).json()['features'])    ## Download the metadata\n",
    "#meta.columns = df.columns.str.replace('geometry.', '', regex=False).str.replace('properties.', '', regex=False)  # clean up column headers \n",
    "\n",
    "# Crreate a list of the UH SLC reservoirs to loop over \n",
    "UHSLC_ID = meta['id'].tolist() \n",
    "UHSLC_ID = [x for x in UHSLC_ID if str(x) != 'nan']  # Filter out nan (Not a Number) values from the list UHSLC_ID.  \n",
    "\n",
    "# Loop over each reservoir to save each of the files in a date stamped folder\n",
    "for ID in UHSLC_ID: \n",
    "    data = pd.read_csv('https://uhslc.soest.hawaii.edu/reservoir/'  + ID  +  '-full.csv')\n",
    "    data.to_csv(os.path.join(Datapath, \"{}.csv\".format(ID)))\n",
    "    print(ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151b582",
   "metadata": {},
   "source": [
    "## Pulling data from USGS gauges, \n",
    "\n",
    "Brian here is the basic function, can you clean up and maybe wrap this in to what we have already in terns of the known reservoir ID's and the right date ranges? \n",
    "\n",
    "\n",
    "Then a key issue, if you make files over 100 mb it will F-up github, maybe keep the date ranges small for now and we can figure out a system to keep the files ignored so they dont mess up the repo later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b8a3e716-8368-40c4-98bf-3a48ac09b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16208400\n",
      "16094150\n",
      "16210000\n",
      "213320158061401\n",
      "213133158014201\n",
      "213308158035601\n",
      "16206600\n"
     ]
    }
   ],
   "source": [
    "#read in USGS id and start dates\n",
    "usgs_id = pd.read_excel('USGS_ID_Meta.xlsx')\n",
    "usgs_id = usgs_id.set_index('USGS station number')\n",
    "\n",
    "#designate end date \n",
    "end_date = today\n",
    "\n",
    "#pull waterlevel data into dictionary\n",
    "USGS_WL_data = {}\n",
    "for id in usgs_id.index: \n",
    "    # Define the site and parameters\n",
    "    site = id\n",
    "    start = str(usgs_id.BEGIN[id]).split(' ')[0]\n",
    "    end = end_date\n",
    "    parameter = \"00065\"  # Stage/streamflow\n",
    "    \n",
    "    # Define the USGS URL for the instantaneous values (IV) service\n",
    "    url = f\"https://waterservices.usgs.gov/nwis/iv/?format=json&sites={site}&startDT={start}&endDT={end}&parameterCd={parameter}&siteStatus=all\"\n",
    "    \n",
    "    # Request the data\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    # Loop over all the time series\n",
    "    for time_series in data['value']['timeSeries']:\n",
    "        # Loop over each value entry within the time series\n",
    "        for entry in time_series['values'][0]['value']:\n",
    "            # Extract the needed information and store it in a dictionary\n",
    "            row = {\n",
    "                \"dateTime\": entry[\"dateTime\"],\n",
    "                \"value\": float(entry[\"value\"]),\n",
    "                \"qualifiers\": entry[\"qualifiers\"]\n",
    "            }\n",
    "            # Append the row to the list of rows\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Convert the list of rows into a pandas DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    USGS_WL_data[id] = df\n",
    "    \n",
    "\n",
    "\n",
    "#set up folder for saving data\n",
    "# First Set up the loacation of where to put the data\n",
    "today = datetime.today().strftime('%Y-%m-%d')            # new data for each date so record the date\n",
    "os.makedirs(os.path.join(\"..\", \"Data\", \"WL_Data{}\".format(today), \"USGS\"), exist_ok=True) # create a new folder for todays date \n",
    "\n",
    "# Definte the folder to save files into \n",
    "Datapath = os.path.join(\"..\", \"Data\", \"WL_Data{}\".format(today), \"USGS\")\n",
    "\n",
    "for ID in USGS_WL_data: \n",
    "    data = USGS_WL_data[ID]\n",
    "    data.to_csv(os.path.join(Datapath, \"{}.csv\".format(ID)))\n",
    "    print(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43642943-30fc-4d32-a15f-6747393dd94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
