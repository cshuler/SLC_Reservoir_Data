{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68e887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import datetime\n",
    "from dateutil import parser, rrule\n",
    "from datetime import datetime, time, date\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fee76",
   "metadata": {},
   "source": [
    "# RAINDATA\n",
    "using statewide data download from HCDP, its actually a total pain. The data formattng is strange, and station metadata is inconsistent. Oh well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec64bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Formats rainfall data and metadata from CSV files, organizing the rainfall data by station and generating a metadata summary for each station.\n",
    "\n",
    "Args:\n",
    "    raindatapath (str): Path to the directory containing the rainfall data CSV files.\n",
    "\n",
    "Returns:\n",
    "    tuple: \n",
    "        - pd.DataFrame: A DataFrame containing the formatted rainfall data, where each column corresponds to a station and rows represent dates.\n",
    "        - pd.DataFrame: A DataFrame containing the metadata for each station, including station ID, coordinates, and other relevant information.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def FormatHCDP_Rain_Data(raindatapath):\n",
    "\n",
    "    Datadic = {}\n",
    "    metadata_dic = {}\n",
    "    \n",
    "    for year in os.listdir(os.path.join(\"..\", raindatapath)):\n",
    "        for month in os.listdir(os.path.join(\"..\", raindatapath, year)):\n",
    "            file = os.listdir(os.path.join(\"..\", raindatapath, year, month))\n",
    "\n",
    "            # Process into a better format where columns are individual stations and y axis is date\n",
    "            data = pd.read_csv(os.path.join(\"..\", raindatapath, year, month, file[0]))\n",
    "\n",
    "            # Extract a separaate dataframe of metadata \n",
    "            data_metaonly = data[['SKN', 'Station.Name', 'Observer', 'Network', 'Island', 'ELEV.m.',\n",
    "           'LAT', 'LON', 'NCEI.id', 'NWS.id', 'NESDIS.id', 'SCAN.id', 'SMART_NODE_RF.id']]\n",
    "            metadata_dic[file[0]]  = data_metaonly\n",
    "\n",
    "\n",
    "            data.drop([ 'Station.Name', 'Observer', 'Network', 'Island', 'ELEV.m.',\n",
    "                   'LAT', 'LON', 'NCEI.id', 'NWS.id', 'NESDIS.id', 'SCAN.id',\n",
    "                   'SMART_NODE_RF.id'], axis=1, inplace=True)\n",
    "\n",
    "            data = data.set_index(\"SKN\", drop=True)\n",
    "            data = data.transpose()\n",
    "\n",
    "            data = data.reset_index()\n",
    "            data['index'] = data['index'].astype(str)\n",
    "\n",
    "            data['index'] = data['index'].apply(lambda x: x[1:])  # Pull out the X\n",
    "            data['index'] = pd.to_datetime(data['index'], yearfirst=True)  # .dt.strftime(\"%Y-%m-%d\")\n",
    "            data.set_index(\"index\", inplace=True)\n",
    "            data.columns.name = None   # drop the wierd double axis label \n",
    "\n",
    "            Datadic[file[0]]  = data \n",
    "\n",
    "    # Produce the rainfall dataframe \n",
    "    RainData = pd.concat(Datadic.values(), sort=True)  \n",
    "    # Produce the metadataframe \n",
    "    RainData_Meta = pd.concat(metadata_dic.values(), sort=True) \n",
    "\n",
    "    # SO herees a problem, NO field in the metadata is unique, meaning there is some f-up in the station IDentifiers.  Creating a *hoprefully* unique ID with a couple fields\n",
    "    RainData_Meta['UID'] = RainData_Meta['LAT'].map('{:.6f}'.format).astype(str)+\"_\"+RainData_Meta['LON'].map('{:.6f}'.format).astype(str)\n",
    "    RainData_Meta.drop_duplicates(subset='UID', inplace=True)     # Hoping to limit to to non-duplicate stations    \n",
    "\n",
    "    # genearate near table outside in Arc \n",
    "    RainData_Meta[\"SKN\"] = RainData_Meta[\"SKN\"].astype(str)+\"_R\"   # First make Arc read it as a string \n",
    "    RainData_Meta.to_csv('RainData_Meta_v2.csv')\n",
    "    # NOW Open in Arc and do a \"Point Distance\" to generate a near table for each of the reservoirs and each rain location point\n",
    "    # Then need to filter pertinant fields and so a table join based on the table FIDs\n",
    "    RainData_Meta.head()\n",
    "\n",
    "    return RainData, RainData_Meta\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ffd0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Function\n",
    "raindatapath = \"Data/HCDP_Rainfall_Daily_Data/station_data\"\n",
    "\n",
    "RainData, RainData_Meta = FormatHCDP_Rain_Data(raindatapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b14fb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RainData.to_csv(os.path.join(\"..\", 'Data/HCDP_Rainfall_Daily_Data', \"HCDP_RainData2018-2024.csv\"))\n",
    "RainData_Meta.to_csv(os.path.join(\"..\", 'Data/HCDP_Rainfall_Daily_Data', \"HCDP_Rain_METADATA_2018-2024.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0eeb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b8d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
